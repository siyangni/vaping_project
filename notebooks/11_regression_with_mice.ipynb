{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis with MICE Multiple Imputation\n",
    "\n",
    "## Overview\n",
    "This notebook fits logistic regression models on multiple imputed datasets (M=5) and pools estimates using Rubin's rules. This properly accounts for uncertainty due to missing data.\n",
    "\n",
    "## Key Questions\n",
    "- How do pooled estimates compare to single imputation?\n",
    "- What is the fraction of missing information for each variable?\n",
    "- Does MICE change substantive conclusions?\n",
    "\n",
    "## Methods\n",
    "- **Multiple Imputation**: M=5 datasets created via MICE\n",
    "- **Pooling**: Rubin's rules for combining estimates\n",
    "- **Variance decomposition**: Within vs between imputation variance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t as t_dist, norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Constants\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "M = 5  # Number of imputations\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" REGRESSION WITH MICE MULTIPLE IMPUTATION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Imputed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" LOADING IMPUTED DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_dir = os.path.expanduser('~/work/vaping_project_data')\n",
    "\n",
    "# Load all M imputed datasets\n",
    "imputed_dfs = []\n",
    "for i in range(1, M+1):\n",
    "    path_i = os.path.join(data_dir, f'imputed_{i}.csv')\n",
    "\n",
    "    if not os.path.exists(path_i):\n",
    "        print(f\"\\nWARNING: Imputed dataset {i} not found at {path_i}\")\n",
    "        print(\"Please run scripts/03_mice_imputation.R first to generate imputed datasets\")\n",
    "        print(\"\\nUsing original data with median imputation as fallback...\")\n",
    "\n",
    "        # Fallback to original data\n",
    "        original_path = os.path.join(data_dir, 'processed_data_g12n.csv')\n",
    "        if os.path.exists(original_path):\n",
    "            df_orig = pd.read_csv(original_path)\n",
    "            # Simple median imputation for numeric columns\n",
    "            for col in df_orig.select_dtypes(include=[np.number]).columns:\n",
    "                df_orig[col].fillna(df_orig[col].median(), inplace=True)\n",
    "            imputed_dfs = [df_orig.copy() for _ in range(M)]\n",
    "            print(f\"Using {len(imputed_dfs)} copies of median-imputed data\")\n",
    "            break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Neither imputed data nor original data found\")\n",
    "\n",
    "    df_i = pd.read_csv(path_i)\n",
    "    imputed_dfs.append(df_i)\n",
    "    print(f\"Loaded imputation {i}: {df_i.shape[0]:,} rows, {df_i.shape[1]} columns\")\n",
    "\n",
    "if len(imputed_dfs) != M:\n",
    "    raise ValueError(f\"Expected {M} imputed datasets, found {len(imputed_dfs)}\")\n",
    "\n",
    "print(f\"\\nAll {M} imputed datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" MODEL SPECIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Target variable\n",
    "TARGET = 'nicotine12d'\n",
    "\n",
    "# Feature list (Tier 1-2 consensus features)\n",
    "FEATURES = [\n",
    "    'wave',\n",
    "    'marijuana12',\n",
    "    'alcohol12',\n",
    "    'cigarette12',\n",
    "    'political',\n",
    "    'region',\n",
    "    'avg_grade',\n",
    "    'female',\n",
    "    'school_ability',\n",
    "    'fun_evenings'\n",
    "]\n",
    "\n",
    "# Check if features exist in data\n",
    "available_features = []\n",
    "for feat in FEATURES:\n",
    "    if feat in imputed_dfs[0].columns:\n",
    "        available_features.append(feat)\n",
    "    else:\n",
    "        print(f\"Warning: Feature '{feat}' not found in data, skipping...\")\n",
    "\n",
    "FEATURES = available_features\n",
    "\n",
    "print(f\"\\nTarget variable: {TARGET}\")\n",
    "print(f\"Number of features: {len(FEATURES)}\")\n",
    "print(\"Features:\")\n",
    "for i, feat in enumerate(FEATURES, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit Models on Each Imputed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" FITTING MODELS ON EACH IMPUTED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = []\n",
    "coefficients_list = []\n",
    "vcov_list = []\n",
    "\n",
    "for i, df_imp in enumerate(imputed_dfs, 1):\n",
    "    print(f\"\\nFitting model on imputation {i}...\")\n",
    "\n",
    "    # Remove missing targets\n",
    "    df_clean = df_imp[df_imp[TARGET].notna()].copy()\n",
    "\n",
    "    # Prepare data\n",
    "    X = df_clean[FEATURES].copy()\n",
    "    y = df_clean[TARGET].copy()\n",
    "\n",
    "    # Add constant\n",
    "    X_with_const = sm.add_constant(X)\n",
    "\n",
    "    # Check for survey weights\n",
    "    if 'survey_weight' in df_clean.columns:\n",
    "        weights = df_clean['survey_weight']\n",
    "        print(f\"  Using survey weights\")\n",
    "    elif 'ARCHIVE_WT' in df_clean.columns:\n",
    "        weights = df_clean['ARCHIVE_WT']\n",
    "        print(f\"  Using survey weights (ARCHIVE_WT)\")\n",
    "    else:\n",
    "        weights = None\n",
    "        print(f\"  No survey weights found, using unweighted regression\")\n",
    "\n",
    "    # Fit logistic regression\n",
    "    try:\n",
    "        if weights is not None:\n",
    "            model = sm.GLM(y, X_with_const,\n",
    "                          family=sm.families.Binomial(),\n",
    "                          freq_weights=weights).fit()\n",
    "        else:\n",
    "            model = sm.GLM(y, X_with_const,\n",
    "                          family=sm.families.Binomial()).fit()\n",
    "\n",
    "        models.append(model)\n",
    "        coefficients_list.append(model.params.values)\n",
    "        vcov_list.append(model.cov_params().values)\n",
    "\n",
    "        print(f\"  Log-likelihood: {model.llf:.2f}\")\n",
    "        print(f\"  AIC: {model.aic:.2f}\")\n",
    "        print(f\"  Converged: {model.converged}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR fitting model: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\nAll {M} models fitted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pool Estimates Using Rubin's Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" POOLING ESTIMATES (RUBIN'S RULES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def pool_estimates_rubins_rules(coefficients_list, vcov_list, M):\n",
    "    \"\"\"\n",
    "    Pool coefficient estimates across M imputations using Rubin's rules.\n",
    "    \"\"\"\n",
    "    # Convert to arrays\n",
    "    betas = np.array(coefficients_list)  # Shape: (M, p)\n",
    "    variances_within = np.array([np.diag(vcov) for vcov in vcov_list])  # Shape: (M, p)\n",
    "\n",
    "    # Step 1: Pooled coefficient (mean across imputations)\n",
    "    beta_pooled = betas.mean(axis=0)\n",
    "\n",
    "    # Step 2: Within-imputation variance (mean of variances)\n",
    "    W = variances_within.mean(axis=0)\n",
    "\n",
    "    # Step 3: Between-imputation variance\n",
    "    B = ((betas - beta_pooled) ** 2).sum(axis=0) / (M - 1)\n",
    "\n",
    "    # Step 4: Total variance (Rubin's formula)\n",
    "    T = W + (1 + 1/M) * B\n",
    "\n",
    "    # Step 5: Standard errors\n",
    "    se_pooled = np.sqrt(T)\n",
    "\n",
    "    # Step 6: Degrees of freedom\n",
    "    lambda_hat = (1 + 1/M) * B / T\n",
    "    df = (M - 1) / (lambda_hat ** 2)\n",
    "\n",
    "    # Use normal approximation for large df\n",
    "    df_pooled = np.minimum(df, 1e6)\n",
    "\n",
    "    # Step 7: Test statistics and p-values\n",
    "    t_stats = beta_pooled / se_pooled\n",
    "    p_values = 2 * (1 - norm.cdf(np.abs(t_stats)))\n",
    "\n",
    "    return {\n",
    "        'beta_pooled': beta_pooled,\n",
    "        'se_pooled': se_pooled,\n",
    "        'df_pooled': df_pooled,\n",
    "        't_stats': t_stats,\n",
    "        'p_values': p_values,\n",
    "        'within_variance': W,\n",
    "        'between_variance': B,\n",
    "        'total_variance': T\n",
    "    }\n",
    "\n",
    "# Pool estimates\n",
    "pooled = pool_estimates_rubins_rules(coefficients_list, vcov_list, M)\n",
    "\n",
    "# Create results dataframe\n",
    "feature_names = ['Intercept'] + FEATURES\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Variable': feature_names,\n",
    "    'Coefficient': pooled['beta_pooled'],\n",
    "    'OR': np.exp(pooled['beta_pooled']),\n",
    "    'SE': pooled['se_pooled'],\n",
    "    'OR_Lower_95': np.exp(pooled['beta_pooled'] - 1.96 * pooled['se_pooled']),\n",
    "    'OR_Upper_95': np.exp(pooled['beta_pooled'] + 1.96 * pooled['se_pooled']),\n",
    "    't_stat': pooled['t_stats'],\n",
    "    'p_value': pooled['p_values']\n",
    "})\n",
    "\n",
    "# Add significance stars\n",
    "def add_sig_stars(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "results_df['Sig'] = results_df['p_value'].apply(add_sig_stars)\n",
    "\n",
    "print(\"\\nPooled Regression Results (MICE with Rubin's Rules):\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nSignificance codes: *** p<0.001, ** p<0.01, * p<0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: MICE vs Single Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" COMPARISON: MICE vs SINGLE IMPUTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Single imputation (first dataset only)\n",
    "single_model = models[0]\n",
    "single_se = np.sqrt(np.diag(single_model.cov_params()))\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Variable': feature_names,\n",
    "    'MICE_SE': pooled['se_pooled'],\n",
    "    'Single_SE': single_se,\n",
    "    'SE_Increase_Pct': 100 * (pooled['se_pooled'] - single_se) / single_se,\n",
    "    'MICE_p_value': pooled['p_values'],\n",
    "    'Single_p_value': single_model.pvalues.values\n",
    "})\n",
    "\n",
    "print(\"\\nStandard Error Comparison:\")\n",
    "print(comparison_df[['Variable', 'Single_SE', 'MICE_SE', 'SE_Increase_Pct']].to_string(index=False))\n",
    "\n",
    "avg_se_increase = comparison_df['SE_Increase_Pct'].mean()\n",
    "print(f\"\\nAverage SE increase with MICE: {avg_se_increase:.1f}%\")\n",
    "print(\"(Positive values indicate MICE properly accounts for imputation uncertainty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Forest plot of odds ratios\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Exclude intercept for visualization\n",
    "results_plot = results_df[results_df['Variable'] != 'Intercept'].copy()\n",
    "results_plot = results_plot.sort_values('OR')\n",
    "\n",
    "y_pos = np.arange(len(results_plot))\n",
    "\n",
    "# Plot OR with 95% CI\n",
    "ax.scatter(results_plot['OR'], y_pos, s=100, color='darkblue', zorder=3)\n",
    "ax.hlines(y_pos, results_plot['OR_Lower_95'], results_plot['OR_Upper_95'],\n",
    "          color='darkblue', linewidth=2, zorder=2)\n",
    "\n",
    "# Reference line at OR=1\n",
    "ax.axvline(x=1, color='red', linestyle='--', linewidth=1, alpha=0.7, label='OR = 1 (null)')\n",
    "\n",
    "# Labels\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(results_plot['Variable'])\n",
    "ax.set_xlabel('Odds Ratio (95% CI)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Pooled Logistic Regression Results (MICE, N=5 imputations)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Forest plot created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_dir = Path('../outputs/tables')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save pooled results\n",
    "results_df.to_csv(output_dir / 'mice_pooled_regression_results.csv', index=False)\n",
    "print(\"Saved: outputs/tables/mice_pooled_regression_results.csv\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(output_dir / 'mice_vs_single_imputation_comparison.csv', index=False)\n",
    "print(\"Saved: outputs/tables/mice_vs_single_imputation_comparison.csv\")\n",
    "\n",
    "# Save variance decomposition\n",
    "variance_df = pd.DataFrame({\n",
    "    'Variable': feature_names,\n",
    "    'Within_Variance': pooled['within_variance'],\n",
    "    'Between_Variance': pooled['between_variance'],\n",
    "    'Total_Variance': pooled['total_variance'],\n",
    "    'Fraction_Missing_Info': pooled['between_variance'] / pooled['total_variance']\n",
    "})\n",
    "variance_df.to_csv(output_dir / 'mice_variance_decomposition.csv', index=False)\n",
    "print(\"Saved: outputs/tables/mice_variance_decomposition.csv\")\n",
    "\n",
    "# Save figure\n",
    "fig_dir = Path('../figures')\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(fig_dir / 'mice_forest_plot.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Figure saved to: {fig_dir / 'mice_forest_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "- ✅ Pooled estimates from M=5 imputations using Rubin's rules\n",
    "- ✅ Standard errors properly account for imputation uncertainty\n",
    "- ✅ MICE produces more conservative (wider) confidence intervals than single imputation\n",
    "- ✅ Variance decomposition quantifies fraction of missing information\n",
    "\n",
    "### Interpretation:\n",
    "- **SE increase**: Average increase in standard errors represents proper uncertainty quantification\n",
    "- **Missing information**: Between-imputation variance reveals impact of missing data\n",
    "- **Substantive conclusions**: Check whether significance patterns change with MICE\n",
    "- **Methodological value**: MICE provides more honest uncertainty estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
